{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d08a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bike Sharing Assignment\n",
    "# import depended libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "### 1. Data Reading & Understanding\n",
    "# Loading the loan data and converting into dataframe\n",
    "df=pd.read_csv(\"day.csv\")\n",
    "df\n",
    "df.shape\n",
    "# To check if there is any headers/footers or summary details rows are there and delete it.\n",
    "df.head() # summay about the loan Data\n",
    "print(df.shape)\n",
    "df.head()\n",
    "df.tail() # checking if totaol, subtotoal rows present\n",
    "# From above analysis We can see that there is no header/footers/summary/subtotal rows\n",
    "# Also number of rows is 730 and columns is 16.\n",
    "\n",
    "# To check if there is any null values.\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "# We can see that there is no null values present in data.\n",
    "\n",
    "# Here data looks with in the permissiable ranage.\n",
    "df.describe()\n",
    "#Delete  Unnessasry columns:\n",
    "# 1. From above analysis we can see that the columns instant is unique in nature and act like a id which will not participate\n",
    "#in analysis. Hence we can drop them.\n",
    "# we can also see that only dtedat is of object type while others attribute is of int/float type.\n",
    "# Since we already having month and year so we can drop dteday columns.\n",
    "\n",
    "print(\"Total columns with values NA: \", len(df.columns[df.isna().all()].tolist()))\n",
    "\n",
    " #Lets delete all the columns which value is uniqe in nature.\n",
    "\n",
    "unique_columns = [col for col in df.columns if df[col].nunique() == len(df)]\n",
    "unique_columns\n",
    "dropped_columns=['instant','dteday']\n",
    "df.drop(dropped_columns,axis=1,inplace=True)\n",
    "df.shape\n",
    "\n",
    "# Checking  if there any duplicates row. \n",
    "duplicate_rows = len(df[df.duplicated()])\n",
    "print(\"Count of duplicate rows:\",round(duplicate_rows,2))\n",
    "# No duplicaets rows found.\n",
    "\n",
    "df.nunique().sort_values()\n",
    "# so there is no any more columns with uniqe values.\n",
    "# Inferences:\n",
    "# We can see that values are in with the ranges. So need to remove any outliers.\n",
    "# Transforming the Data\n",
    "# Form data dictionary and abvoe analysis we can say that  season, weathersit & month are catogorical values but found as int.\n",
    "#Lets Convert season, weathersit & month to Catogory type\n",
    "df['season']=df['season'].astype('category')\n",
    "df['weathersit']=df['weathersit'].astype('category')\n",
    "df['mnth']=df['mnth'].astype('category')\n",
    "df.info()\n",
    "# As per data dictionary, following are the categorical variables has following values.\n",
    "# 1. yr has two values 0 for 2018 and 1 for 2019.\n",
    "# 2. holiday o or 1.\n",
    "# 3. working day 0 or 1.\n",
    "# 4. weathersit has three values good, bad and moderate weatherSit. \n",
    "# 5. season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "# all the data seems to be in range\n",
    "df.season.replace({1:\"spring\", 2:\"summer\", 3:\"fall\", 4:\"winter\"},inplace = True)\n",
    "df.weathersit.replace({1:'good',2:'moderate',3:'bad',4:'severe'},inplace = True)\n",
    "\n",
    "df.mnth = df.mnth.replace({1: 'jan',2: 'feb',3: 'mar',4: 'apr',5: 'may',6: 'jun',\n",
    "                  7: 'jul',8: 'aug',9: 'sept',10: 'oct',11: 'nov',12: 'dec'})\n",
    "\n",
    "df.weekday = df.weekday.replace({0: 'sun',1: 'mon',2: 'tue',3: 'wed',4: 'thu',5: 'fri',6: 'sat'})\n",
    "df.head()\n",
    "### 2. EDA\n",
    "#### Univariate analysis\n",
    "# Lets check if any outliers present is numeric featires.\n",
    "# raw box plots for indepent variables with continuous values\n",
    "cols = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "plt.figure(figsize=(18,4))\n",
    "\n",
    "k = 1\n",
    "for col in cols:\n",
    "    plt.subplot(1,4,k)\n",
    "    sns.boxplot(y=col, data=df)\n",
    "    k+=1\n",
    "# Line graph will that show the count vs month for year 2018 and 2019.\n",
    "sns.lineplot(x = \"mnth\", y = \"cnt\", data=df,hue = \"yr\")\n",
    "plt.show() \n",
    "# Inferences:\n",
    "# 1. From Line Graph we can see that target Variable cnt has increased from 2018 to 2019.\n",
    "# 2. Demand for bike is high between may to oct for year 2018 and 2019.\n",
    "# 3. From Boxchart we can also see that there is no outliers present.\n",
    "# Lets plot pairplots to have better to see if cnt is linearly  depend upon is independet variables. \n",
    "numerical_variabels=['cnt', 'temp', 'atemp', 'hum','windspeed']\n",
    "sns.pairplot(data=df,vars=numerical_variabels, kind=\"reg\")\n",
    "plt.show()\n",
    "# Inferences:\n",
    "# 1. We can see that we have linealy dependecy between temp, atemp and count.\n",
    "# 2. we can also see that temp and atemp is highly correlated.\n",
    "####  Bivariate analysis\n",
    "#  Barcharts for categorical variables to see demands\n",
    "# function to create barplot related to categorical columns\n",
    "\n",
    "def plot_bar_graphs(column):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=column, y='cnt', data=df)\n",
    "    plt.title(f'Bar Plot of {column} vs cnt')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=column, y='cnt', data=df, hue='yr', palette='Set1')\n",
    "    plt.title(f'Bar Plot of {column} vs cnt (2018 vs 2019)')\n",
    "    plt.legend(title='yr', labels=['2018', '2019'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_bar_graphs('season')\n",
    "# Inferences:\n",
    "# 1. Fall  seasons is having high number bike rental.\n",
    "# 2. Bike rental in 2019 has increaded for every seasion compared to 2018.\n",
    "\n",
    "plot_bar_graphs('mnth')\n",
    "# Inferences:\n",
    "# 1. May to oct is having number of bike registration.\n",
    "# 2. Bike registration in 2019 has increaded for every month compared to 2018.\n",
    "plot_bar_graphs('weathersit')\n",
    "# Inferences:\n",
    "# 1. People prefer bike rental when weather situations is good. \n",
    "# 2. Bike registration in 2019 has increaded  for every weather situtation 2018.\n",
    "plot_bar_graphs('weekday')\n",
    "# Inferences:\n",
    "# 1. People don't prefer bike rental on weekends. \n",
    "\n",
    "plot_bar_graphs('holiday')\n",
    "\n",
    "# Inferences:\n",
    "# 1. People dont prefer renting bike holiday.\n",
    "\n",
    "plot_bar_graphs('workingday')\n",
    "# Inferences:\n",
    "# 1. People prefer bike rental on working days.\n",
    "# Lets draw heatMap to indentify is there any correlation.\n",
    "\n",
    "# Lets plot the corrlation matrix(heatmap)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.heatmap(df.corr(), cmap='BuGn', annot = True)\n",
    "plt.show()\n",
    "#Inferences from heatmap: \n",
    "#1. Since Count is sum of casual and registed. That we can infer from  heatmap as casual and registered are highly correleted with cnt. \n",
    "#2. holiday, hum, and windspeed is negatively correlated. \n",
    "#3. Indepedent variable temp and atemp is highly correlated(0.99).This show that there is high possiblity that one may have derived from another,One of the them can be dropped. Will use VIP and p values to drop this.\n",
    "df.shape\n",
    "# Dropping casual and Registed as  they sumed to Cnt.\n",
    "\n",
    "df.drop(['casual','registered'],axis=1,inplace=True)\n",
    "\n",
    "df.shape\n",
    "## 3. Data Preparation\n",
    "df.info()\n",
    "#### Creating dummy variables for catogorical variables.\n",
    "# we need to create a dummy variables for four categorical variables. mnth', 'weekday', 'season' & 'weathersit\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "df.head()\n",
    "# Divide the train and test data\n",
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(df, train_size=0.75, test_size=0.25, random_state=100)\n",
    "df_train.shape\n",
    "\n",
    "df_test.shape\n",
    "#### Scaling numeric variables \n",
    "#scaling numeric variables of traning data using MinMaxScalor between 0-1.\n",
    "num_vars =  ['temp', 'atemp', 'hum', 'windspeed','cnt']\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "df_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n",
    "df_train.head()\n",
    "df_train.describe()\n",
    "# From above analysis we can see that numeric variabels has scaled between 0 and 1.\n",
    "\n",
    "# Plotting a correlation matrix\n",
    "plt.figure(figsize = (40, 40))\n",
    "sns.heatmap(df_train.corr(), annot = True, cmap=\"BuGn\")\n",
    "plt.show()\n",
    "# poping seperating independent and dependent variables\n",
    "y_train=df_train.pop('cnt')\n",
    "X_train=df_train\n",
    "\n",
    "## 4. Build a Model\n",
    "####  Model #1 \n",
    "# Building a model with all features using statsmodels :\n",
    "import statsmodels.api as sm\n",
    "X_train_lm = sm.add_constant(X_train)\n",
    "\n",
    "lr1 = sm.OLS(y_train, X_train_lm).fit()\n",
    "\n",
    "lr1.params\n",
    "lr1.summary()\n",
    "# Here, we can see that all adjusted R-Squire is 0.848 is pretty good.Lets us see if we can reduce the independet features\n",
    "# Since number of predictor variables is around 30. It will not be feasible to manually eliminate the insignificance variable.\n",
    "# Will use hybrid (REF + manual) to elimnate insignifiance variable.\n",
    "# Importing RFE and LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Running RFE with the output number of the variable equal to 15\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(estimator=lm, n_features_to_select=15)   # running RFE\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns,rfe.support_,rfe.ranking_))\n",
    "# RFE support Variables\n",
    "col = X_train.columns[rfe.support_]\n",
    "print(len(col))\n",
    "col\n",
    "# RFE not Supported Variables\n",
    "X_train.columns[~rfe.support_]\n",
    "# Lets consider only  the RFE supported columns for model building \n",
    "X_train_rfe = X_train[col]\n",
    "\n",
    " #Model#2 \n",
    "# Generic function to calculate VIF of variables\n",
    "\n",
    "def calculateVIF(df):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = df.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return vif \n",
    "calculateVIF(X_train_rfe)\n",
    "# Model 2 \n",
    "#Add a constant\n",
    "X_train_lm2 = sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr2 = sm.OLS(y_train, X_train_lm2).fit()\n",
    "lr2.summary()\n",
    "# From abvoe analysis all the variables is having low p-values and also adjusted-R-squire value is 0.847 which is been\n",
    "# increased from 0.843 to 0.847 after droping 14 features, this means most of the features were insignifiance and few were negative collrelatin\n",
    "# from above VIP table we can see that workingday is having 31.53 vip value which is quite high. lets remove this featues.\n",
    "\n",
    "X_train_rfe2 = X_train_rfe.drop([\"workingday\"], axis = 1)\n",
    "X_train_rfe2.head()\n",
    "calculateVIF(X_train_rfe2)\n",
    "# Model 3 \n",
    "#Add a constant\n",
    "X_train_lm3 = sm.add_constant(X_train_rfe2)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr3 = sm.OLS(y_train, X_train_lm3).fit()\n",
    "lr3.summary()\n",
    "# From above we can see that weekday_sat is having high pValue 0.482>0.05  and hum VIP is 14.57>5.\n",
    "# Lets drop weekday_sat\n",
    "X_train_rfe3 = X_train_rfe2.drop([\"weekday_sat\"], axis = 1)\n",
    "X_train_rfe3.head()\n",
    "\n",
    "calculateVIF(X_train_rfe3)\n",
    "\n",
    "# Model 4\n",
    "#Add a constant\n",
    "X_train_lm4 = sm.add_constant(X_train_rfe3)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr4 = sm.OLS(y_train, X_train_lm4).fit()\n",
    "lr4.summary()\n",
    "# we can see that Adj. R-squire is 0.847 and pvalue is less than 0.05 for every features.\n",
    "# VIP for hum is 14.57>5.so Lets frist remove the hum.\n",
    "\n",
    "X_train_rfe4 = X_train_rfe3.drop([\"hum\"], axis = 1)\n",
    "X_train_rfe4.head()\n",
    "calculateVIF(X_train_rfe4)\n",
    "\n",
    "X_train_lm5 = sm.add_constant(X_train_rfe4)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr5 = sm.OLS(y_train, X_train_lm5).fit()\n",
    "lr5.summary()\n",
    "#From the above  Model Summary Report we can say that all the p-values of respective features are less than 0.05.\n",
    "# Note: It is a hurestic approach that, if VIF value of any predictor is between 5 & 10, it is recommended to scrutinize such variable from business perspective and analyse any outliers & drop such feature if VIF is greater than 10\n",
    "# So we can now fix this \n",
    "### 5. Linear Regrssion Assumptions Proofs\n",
    "#### Residual Analysis\n",
    "# Lets perform Residual Analysis\n",
    "y_train_pred = lr5.predict(X_train_lm5)\n",
    "residual = y_train_pred - y_train\n",
    "#1. Assumption of Normally Distributed Error Term\n",
    "\n",
    "# Plot the histogram of the error terms\n",
    "sns.distplot(residual)\n",
    "plt.xlabel('Residual (Error Term)', fontdict={'fontsize': 20})\n",
    "plt.title('Error Distribution', fontdict={'fontsize': 25})\n",
    "plt.show() \n",
    "\n",
    "#Inferences: From the above graph it is evident that Error Distribution Is Normallly Distributed Across 0, which indicates that our model has handled the assumption of Error Normal Distribution properly.\n",
    "\n",
    "#2 Assumption of Error Terms Being Independent\n",
    "\n",
    "sns.regplot(x=y_train_pred, y=residual)\n",
    "plt.title('Residual Vs. Predicted Values (Pattern Indentification)', fontdict={'fontsize': 20})\n",
    "plt.xlabel('Predicted Values', fontdict={'fontsize': 15})\n",
    "plt.ylabel('Residuals', fontdict={'fontsize': 15})\n",
    "plt.show()\n",
    "#From the above graph, we see that there is almost no relation between Residual & Predicted Value\n",
    "# 3. Homoscedasticity\n",
    "\n",
    "sns.regplot(x=y_train, y=y_train_pred)\n",
    "plt.title('Predicted Points Vs. Actual Points', fontdict={'fontsize': 20})\n",
    "plt.xlabel('Actual Points', fontdict={'fontsize': 15})\n",
    "plt.ylabel('Predicted Points', fontdict={'fontsize': 15})\n",
    "plt.show()\n",
    "# we can see that variance is  similar from both end of fitted line.\n",
    "# Multicorrelation\n",
    "calculateVIF(X_train_rfe4)\n",
    "plt.figure(figsize = (20, 20))\n",
    "# Heatmap\n",
    "sns.heatmap(X_train_rfe4.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()\n",
    "# As per above analysis we didn't found multi-colinearity among independedt variables.\n",
    "### 6. Model Prediction on Test Data\n",
    "num_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n",
    "\n",
    "df_test[num_vars] = scaler.transform(df_test[num_vars])\n",
    "df_test.head()\n",
    "df_test.describe()\n",
    "# divie into x and y\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test\n",
    "X_test.info()\n",
    "X_test=X_test[X_train_rfe4.columns]\n",
    "# Adding constant variable to test dataframe\n",
    "X_test_lr5 = sm.add_constant(X_test)\n",
    "X_test_lr5.info()\n",
    "y_test_pred = lr5.predict(X_test_lr5)\n",
    "\n",
    " lr5.params\n",
    "### 7. Generating R-Square\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_test_pred)\n",
    "train_adjuested_r_2=0.841\n",
    "train_r2=0.844\n",
    "# n is number of rows in X\n",
    "\n",
    "n = X_test.shape[0]\n",
    "\n",
    "\n",
    "# Number of features (predictors, p) is the shape along axis 1\n",
    "p = X_test.shape[1]\n",
    "\n",
    "# We find the Adjusted R-squared using the formula\n",
    "\n",
    "test_adjusted_r2 = 1-(1-train_r2)*(n-1)/(n-p-1)\n",
    "test_adjusted_r2\n",
    "# Train R-squire=0.844\n",
    "# Test R-squire=0.7795\n",
    "# Train Adj-R-squire=0.841\n",
    "# Test Adj-R-squire=0.832\n",
    "# That seems a very good models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
